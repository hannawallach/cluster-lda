\documentclass[twoside]{article}

\usepackage[accepted]{aistats2e}

\usepackage[plain]{algorithm}
\usepackage{algpseudocode}

\usepackage{graphicx}
\usepackage{subfigure} 

\usepackage{bm}

\usepackage[tbtags]{amsmath}
\usepackage{amssymb,rotating,multirow,natbib}

\input{macros}

\bibliographystyle{natbib}

\begin{document}

\runningtitle{Supplementary Materials for ``An Alternative Prior Process for Nonparametric Bayesian Clustering'' }

\runningauthor{H.M. Wallach, S.T. Jensen, L. Dicker, and K.A. Heller}

\twocolumn[

\aistatstitle{Supplementary Materials for ``An Alternative Prior Process for Nonparametric Bayesian Clustering'' }

\aistatsauthor{Hanna M. Wallach \And Shane T. Jensen}

\aistatsaddress{Department of Computer Science \\ University of Massachusetts Amherst \And Department of Statistics\\ The Wharton School, University of Pennsylvania } 

\aistatsauthor{Lee Dicker \And Katherine A. Heller}

\aistatsaddress{Department of Biostatistics \\ Harvard School of
  Public Health \And Engineering Department\\ University of Cambridge}

]

\section{Proof of Law for $\mathbb{E}\,(K_N\g{\rm UN})$} \label{UNmomresults}

We start by defining $T_k = \inf\, \{ m > T_{k-1} ;\ X_m \notin \{X_1,
\ldots, X_{m-1} \}\}$.  $T_k$ is the ``waiting time'' (number of
observations needed) until the $k^{\textrm{th}}$ new cluster is
generated by the uniform process.  Under the uniform process, $T_k =
\sum_{i=1}^k \tau_i$ where $\tau_i \sim {\rm Geometric}\,
(\theta\,/\,(\theta + i - 1)$ and the $\tau_i$ variables are
independent, so
\begin{align}
\mathbb{E}\, (T_k) &= \sum\limits_{i=1}^k \frac{\theta + i -
  1}{\theta} = \frac{k^2}{2 \theta} + k \left( 1 - \frac{1}{2 \theta}
\right) \nonumber \\
\intertext{and}
 {\rm Var}\, (T_k) &= \sum\limits_{i=1}^k \frac{(\theta + i - 1)\,(i - 1)}{\theta^2} \label{vTk} \\
 &= \frac{k^3}{3 \theta^2} + k^2 \frac{1}{2\theta} \left( 1 - \frac{1}{\theta} \right) + k \frac{1}{2\theta} \left( \frac{1}{3 \theta} - 1 \right). \nonumber
\end{align}
In terms of $T_k$, $K_N = \max \{ k ;\ T_k \leq N \} = \sum_{k=1}^N
{\rm I}\, (T_k \leq N)$. We first prove a strong law for the
convergence of $T_k$.  Let $\epsilon > 0$.  From Chebychev's
inequality and (\ref{vTk}), we have the following:
\begin{align}
{\rm P}\, \left(|T_k - \mathbb{E}\, (T_k) | > \epsilon  k^2\right) \leq \frac{{\rm Var}\, (T_k) }{\epsilon^2k^4}  \leq \frac{C(\theta,\epsilon)}{k}. \label{Cheby}
\end{align}
From (\ref{Cheby}),
\begin{align*}
{\rm P}\, \left(|T_{k^2} - \mathbb{E}\, (T_{k^2})| > \epsilon k^4 \right) \leq \frac{C(\theta,\epsilon)}{k^2},
\end{align*} 
and so by the Borel-Cantelli lemma, we have ${\rm P}\, \left(|T_{k^2}
- \mathbb{E}\,(T_{k^2})| > \epsilon k^4 \right) = 0.$ Since $\epsilon
> 0$ was chosen arbitrarily, it follows that $\frac{T_{k^2} -
  \mathbb{E}\,(T_{k^2})}{k^4} \to 0$ almost surely and hence
$\frac{T_{k^2}}{k^4} \to \frac{1}{2\theta}$ almost surely. Now, let $m
= \lfloor \sqrt{k} \rfloor$.  Since $T_k$ is increasing, we have:
\begin{align}
\frac{T_{m^2}}{(m + 1)^4} \leq \frac{T_k}{k^2} \leq
\frac{T_{(m + 1)^2}}{m^4}.   \label{subseq}
\end{align}
Since $\frac{m + 1}{m} \to 1$, both sides of the inequality (\ref{subseq}) converge to $(2\theta)^{-1}$ almost surely, and so 
\begin{align}
\frac{T_k}{k^2} \to \frac{1}{2\theta} \hbox{ almost surely.}  \label{strongTk}
\end{align}
The strong law (\ref{strongTk}) implies a strong law for $K_N$ as
follows. $T_{K_N} \leq N < T_{K_N + 1}$ and, consequently,
\begin{align*}
\frac{T_{K_N}}{K_N^2} \leq \frac{N}{K_N^2} < \frac{T_{K_N +
1}}{K_N^2}.
\end{align*}
Since $K_N \to \infty$ almost surely and $T_k\,/\,k^2 \to
1\,/\,(2\theta)$ almost surely, it follows that the left and right
hand side above both converge to $1\,/\,(2\theta)$ almost surely.
Thus, $K_N^2 \,/\, N \to 2\theta$ almost surely and so
\begin{align}
\frac{K_N}{\sqrt{N}} \to \sqrt{2\theta} \hbox{ almost surely.}   \label{strongKn}
\end{align}
From the strong law (\ref{strongKn}) and the dominated convergence
theorem, we have the following:
\begin{align}
\frac{\mathbb{E}\,(K_N)}{N} \to 0.  \label{Kngoingtozero}
\end{align}
Combining (\ref{Kngoingtozero}) with following result from
section~\ref{momentlemma},
\begin{align}
\mathbb{E}\,(K_N^2) = \mathbb{E}\,(K_N) + 2\theta\,(N - \mathbb{E}\,(K_N)). \label{twomoments}
\end{align}
gives us
\begin{align}
\frac{\mathbb{E}\,(K_N^2)}{N} \to 2\theta. \label{Knsquaredgoingtotheta}
\end{align}
Finally, using (\ref{Knsquaredgoingtotheta}) together with Fatou's
lemma and Jensen's inequality, gives us the following:
\begin{align*}
\sqrt{2\theta} \leq \liminf_{N \to \infty} \frac{\mathbb{E}\,(K_N)}{\sqrt{N}} & \leq
\limsup_{N \to \infty} \frac{\mathbb{E}\,(K_N)}{\sqrt{N}} \nonumber\\
&\leq \limsup_{N \to
\infty} \sqrt{\frac{\mathbb{E}\,(K_N^2)}{N}} = \sqrt{2\theta}.
\end{align*}
This then proves the result
\begin{align*}
\frac{\mathbb{E}\,(K_N)}{\sqrt{N}} \to \sqrt{2\theta}
\end{align*}
under the uniform process. 

\section{Result relating $\mathbb{E}\,(K_N)$ to  $\mathbb{E}\,(K_N^2)$} \label{momentlemma}

Recall the definition of $T_k$ from above and now define $M_N = K_N +
1$.  Consider the ``waiting time'' $T_{M_N}$ until the observation
that creates the $(K_N + 1)^{\textrm{th}}$ unique cluster.  We relate
$\mathbb{E}\,(K_N)$ to $\mathbb{E}\,(K_N^2)$ by calculating
$\mathbb{E}\, (T_{M_N})$ in two different ways.  First, observe that
\begin{align*}
\mathbb{E}\, (T_{M_N}) &= \mathbb{E}\,
\left(\sum\limits_{k=1}^\infty \tau_k \cdot {\rm I}\, (k \leq M_N)
\right) \\ &= \frac{\theta -1}{\theta}\,
\sum\limits_{k=1}^\infty {\rm P}\, (k \leq M_N) \\ & \,\,\,\,\,\,\,\, + \frac{1}{\theta}
\sum\limits_{k=1}^\infty k \cdot {\rm P}\, (k \leq M_N) \\ &=
\frac{\theta -1}{\theta}\, \mathbb{E}\, (M_N) + \frac{1}{2 \theta}\,
\mathbb{E}\, (M_N (M_N + 1)),
\end{align*}
which, since $M_N = K_N + 1$, simplifies to 
\begin{align}
\mathbb{E}\, (T_{M_N})  = 1 + \mathbb{E}\, (K_N) \left( 1 + \frac{1}{2 \theta} \right) + \mathbb{E}\, (K_N^2)\, \frac{1}{2 \theta}.   \label{lemmaeq1}
\end{align}
Now $T_{M_N} = N + \sum_{j} {\rm I}\, (M_{N + j} = M_N)$ and so
$\mathbb{E}\, (T_{M_N}) = N + \sum_{j} {\rm P}\, (M_{N + j} = M_N)$
where
\begin{align*}
{\rm P}\,(M_{N + j} = M_N) &= \sum_k {\rm P}\,(T_k \leq N, \ N + j < T_{k + 1}) \\
&= \sum_k {\rm P}\,(M_N = k + 1)\, {\rm P}\,(j < \tau_{k + 1}).
\end{align*}
It follows that
\begin{align*}
\mathbb{E}\,(T_{M_N}) &= n + \sum_j \sum_k {\rm P}\,(M_N = k + 1)\, {\rm P}\,(j < \tau_{k + 1}) \\
&= N + \sum_k {\rm P}\,(M_N = k + 1)\, \mathbb{E}\,(\tau_{k + 1}) \\
&= N + \sum_k {\rm P}\,(K_N = k)\, \frac{k+\theta}{\theta},
\end{align*}
which can be simplified to 
\begin{align}
\mathbb{E}\,(T_{M_N})  = N + 1 + \mathbb{E}\,(K_N)\, \frac{1}{\theta}.   \label{lemmaeq2}
\end{align}
Combining (\ref{lemmaeq1}) and (\ref{lemmaeq2}) gives (\ref{twomoments}): 
\begin{align*}
\mathbb{E}\,(K_N^2) = \mathbb{E}\,(K_N) + 2\theta\,(N - \mathbb{E}\,(K_N)). 
\end{align*}

\begin{algorithm*}[t]
\begin{algorithmic}
\State initialize $l \is 0$ \For{each document $d$ in
  $\mathcal{W}^{\textrm{test}}$} \State initialize $p_d \is 0$
\For{each particle $r = 1$ to $R$} \For{$d' < d$} \State $c^{(r)}_{d'}
\sim P(c^{(r)}_{d'} \g \mathcal{W}^{\textrm{test}}_{<d}, \{
\bc^{(r)}_{< d} \}_{\setminus {d'}}, \mathcal{W}^{\textrm{train}},
\bc^{\textrm{train}},\theta, \bbeta)$ \EndFor \State $p_d \is p_d +
\sum_c P(\bw^{\textrm{test}}_d, c^{(r)}_d \!=\! c \g
\mathcal{W}^{\textrm{test}}_{<d},\bc^{(r)}_{<d},
\mathcal{W}^{\textrm{train}}, \bc^{\textrm{train}},\theta, \bbeta)$
\State $c^{(r)}_d \sim P(c^{(r)}_d \g \bw^{\textrm{test}}_d,
\mathcal{W}^{\textrm{test}}_{<d},\bc^{(r)}_{<d},\mathcal{W}^{\textrm{train}},
  \bc^{\textrm{train}},\theta,\bbeta)$
\EndFor
\State $p_n \is p_n \,/\, R$
\State $l \is l + \log{p_n}$
\EndFor
\State $\log{P(\mathcal{W}^{\textrm{test}} \g
  \mathcal{W}^{\textrm{train}},            
  \bc^{\textrm{train}}, \theta,
  \bbeta)} \simeq l$
\end{algorithmic}
\caption{``Left-to-right'' evaluation algorithm for computing
  $\log{P(\mathcal{W}^{\textrm{test}} \g \mathcal{W}^{\textrm{train}},
  \bc^{\textrm{train}}, \theta, \bbeta)}$.}
\label{alg:ltr}
\end{algorithm*}

\section{Evaluation Algorithm} \label{leftright}

The evaluation algorithm used to approximate
$\log{P(\mathcal{W}^{\textrm{test}} \g \mathcal{W}^{\textrm{train}},
  \bc^{\textrm{train}}, \theta, \bbeta)}$ is based on the
``left-to-right'' evaluation algorithm introduced by
\cite{wallach09evaluation}, adapted to marginalize out test cluster
assignments. Pseudocode is given in algorithm~\ref{alg:ltr}.

\subsubsection*{Acknowledgements}

{

\small

This work was supported in part by the Center for Intelligent
Information Retrieval, in part by CIA, NSA and NSF under NSF grant
\#IIS-0326249, and in part by subcontract \#B582467 from Lawrence
Livermore National Security, LLC, prime contractor to DOE/NNSA
contract \#DE-AC52-07NA27344. Any opinions, findings and conclusions
or recommendations expressed in this material are the authors' and do
not necessarily reflect those of the sponsor.

}

{\small
\vspace{-0.1cm}
\begin{thebibliography}{1}

\bibitem[Wallach \emph{et~al.}(2009)Wallach, Murray, Salakhutdinov, and
  Mimno]{wallach09evaluation}
Wallach, H., Murray, I., Salakhutdinov, R., and Mimno, D. (2009).
\newblock Evaluation methods for topic models.
\newblock In \emph{26th International Conference on Machine
  Learning}.

\end{thebibliography}
}

\end{document}


