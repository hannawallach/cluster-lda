ReviewerID: 3435   	8 	8 	8 	8 	8 	8 	8 	5
Comments to author(s)
The paper presents an alternative the Dirichlet Process. DP is often  
used as a nonparametric prior for clustering, but with a "rich-get- 
richer" cluster size distribution (i.e., the number of clusters of  
size k is O(1/k)). The related Pitman-Yor Process is more flexible but  
has similar asymptotic properties. This has been a standing problem  
for practitioners, since real data may not come from such a  
distribution. The proposed alternative yields approximately uniform  
clusters (i.e., number of clusters of size k is O(1) for all k). The  
Uniform Process has been previously applied to problems in  
computational biology, but seems to be largely unknown in machine  
learning. This paper studies the number of clusters and cluster size  
distribution both theoretically and through simulations for DP, Pitman- 
Yor Process, and UP. Clustering results on a real text data set shows  
that the UP indeed results in better loglikelihood than DP.

It's well known that DP and PY yields "biased" clusters, which affects  
its fit to real data. This paper addresses an important problem in  
nonparametric Bayesian modeling. It gives simple asymptotic results  
and demonstrates the effect with simulations. It also shows that UP  
helps on real data. (Figure 4 seems to suggest that a test document  
is, on average, e^2 = 7.4 times more likely under UP than DP, which  
does not guarantee the UP to be a GOOD fit, but at least shows that it  
is a better fit than DP.)

The paper only gives an operational definition of the uniform process,  
namely, how to sample from it. It would be nice if the paper also  
included a formal definition.

Summary of review
Adequately addresses important problem in nonparametric clustering.  
Convincing theoretical and experimental results.

  ReviewerID: 3779   	4 	4 	5 	7 	6 	6 	5 	7
Comments to author(s)
This paper analyzes priors for "nonparametric" clustering models (in  
the sense of clustering models with a potentially unbounded number of  
clusters). The paper compares the Dirichlet process and Pitman-Yor  
process priors with a "uniform" prior in which a new datapoint is  
assigned to a cluster with a distribution that is uniform over  
existing clusters, and has a parameter similar to the Dirichlet  
process governing the creation of new clusters. Asymptotic results for  
the DP and PYP are summarized, an results for the uniform prior are  
presented. All three models are then compared in a clustering problem,  
where the uniform prior appears to result in better performance.

The basic empirical result is interesting, but otherwise the  
contributions in this paper were fairly limited. Asymptotic results  
for the DP and PYP are well-known, and the results for the uniform  
case are only of interest if it seems like it is generally desirable  
as an alternative to these existing priors. The simulation comparisons  
were likewise not particularly surprising.

I found the discussion of exchangeability for the uniform prior  
confusing. First of all, "exchangeable" is a property of  
distributions, not of cluster assignments. Perhaps talking about  
cluster assignments corresponding to the same partition might be  
clearer. Second, the idea of reducing to a canonical ordering for each  
partition seems problematic, since the distribution no longer sums to  
one. Either way, there isn't a simple way to guarantee infinite  
exchangeability, which is the property that makes the DP and PYP  
attractive.

Summary of review
There are some interesting ideas in this paper, but the overall  
contributions are too weak to make it suitable for publication.

  ReviewerID: 3434   	3 	3 	5 	9 	9 	2 	4 	9
Comments to author(s)
This paper proposes a new prior distribution over partitions. The  
authors show experimentally that this new distribution gives better  
fit for clustering than standard prior such as the Dirichlet process.  
Although the subject is of great interest and the paper is very well  
written, there are several problems with the use of such prior for  
clustering and a few errors in the submitted article.

First of all, in section 2.1, there is a many-to-one relation between  
the X's and the N's. As an example, the sequences X=(1,1,2) and  
(1,2,1) induce the same partition N=(2, 1). Thus the distribution of N  
does not correspond to the joint distribution of (3) as claimed page  
3. Indeed, Eq. (4) is not correct. As a counter example,
Pr(N=(2, 1)) = Pr(X=(1,1,2) ) + Pr(X=(1,2, 1) ) = theta*(2*theta + 3)/  
(theta+1)² /(theta +2)
which is different from Eq. (4).

Contrary to what is written by the authors, lack of exchangeability is  
a serious problem for any "chinese restaurant process" type prior for  
clustering. The bag-of-word assumption is usually made when performing  
clustering, i.e. the labelling of the data is considered as  
irrelevant. However, with the proposed prior, different ordering of  
the data will give different results. Below are two examples to show  
why exchangeability is very desirable for clustering:

* Consider 1000 data and theta=1. Then when simulating from the  
proposed model (3), the data labelled #1 will (a priori) belong to a  
cluster of average size 43 whereas the data labelled #1000 will belong  
to a cluster of average size 24.

* Without exchangeability, we do not have Pr(c_t|c_{-t}) which is  
given by Eq. (3), and Eq. (16) is therefore not correct. As a counter  
example
Pr(c2=1 | c1=1, c3=1) = Pr(c2=1|c1=1)Pr(c3=1|c2=1)/Pr(c1=1,c3=1)
= theta*(theta+2) / (theta+1)
which is different from 1/(theta+1).

Summary of review
This paper proposes a new prior distribution over partitions. The  
authors show experimentally that this new distribution gives better  
fit for clustering than standard prior such as the Dirichlet process.  
Although the subject is of great interest and the paper is very well  
written, there are fundamental problems with the use of such 'non  
exchangeable' prior for clustering and a few errors in the submitted  
article.


ReviewerID: 3436   	5 	7 	3 	8 	7 	7 	4 	9
Comments to author(s)
The paper is well-written and discusses the (obvious) uniform process  
prior for Bayesian clustering.

The paper as such seems to have no new contributions when compared to  
say [7], which already discusses the use of this prior (as does of  
course [12]). The paper then goes on to mention that previous work,  
such as [7] does not really study theoretical properties of this  
prior. However, neither does this paper! The arXiv reference [3]  
includes the sole theoretic resulted related to this prior (in an  
appendix, as it is a fairly routine textbook manipulation from what I  
understand).

Understandably, there do occur clustering scenarios where we want to  
avoid "rich-get-richer" --- but often in these scenarios, we might  
even want to avoid "well-balanced" or similarly sized clusters, which  
the uniform prior seems to favor. However, despite the good  
motivation, the material on the use of this prior has already appeared  
in print, taking away what would be the only contribution of this paper.

Otherwise, the paper is very reasonable.

Summary of review
Natural prior to look at. No novelty, as almost all the important  
parts of this paper have already appeared elsewhere.
